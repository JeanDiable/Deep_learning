{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_batch_1', 'readme.html', 'batches.meta', 'data_batch_2', 'data_batch_5', 'test_batch', 'data_batch_4', 'data_batch_3']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "CIFAR_DIR = \"./cifar-10-batches-py\"\n",
    "print(os.listdir(CIFAR_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3072)\n",
      "(10000,)\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[[-0.08235294  0.03529412 -0.00392157 ...  0.24705882  0.25490196\n",
      "   0.2627451 ]\n",
      " [ 0.52941176  0.56078431  0.59215686 ... -0.03529412 -0.06666667\n",
      "  -0.01960784]\n",
      " [-0.51372549 -0.50588235 -0.51372549 ... -0.36470588 -0.41176471\n",
      "  -0.52156863]\n",
      " ...\n",
      " [ 0.48235294  0.52156863  0.52941176 ...  0.1372549   0.17647059\n",
      "   0.14509804]\n",
      " [ 0.21568627  0.20784314  0.20784314 ... -0.18431373 -0.18431373\n",
      "  -0.17647059]\n",
      " [ 0.44313725  0.23137255  0.25490196 ...  0.3254902   0.30980392\n",
      "   0.24705882]]\n",
      "[1 0 1 1 0 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"load data from data file.\"\"\"\n",
    "    with open(filename,'rb') as f:\n",
    "        data = pickle.load(f,encoding = 'latin')\n",
    "        return data['data'], data['labels']\n",
    "    \n",
    "class CifarData:\n",
    "    \"\"\"定义一个类处理cifar的数据\"\"\"\n",
    "    def __init__(self, filenames, need_shuffle): #对于训练集来说需要shuffle，打乱数据以增加训练成效，对于测试集来说就不需要。\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        for filename in filenames:\n",
    "            data,labels = load_data(filename) \n",
    "            '''this is a 二分类器，so we only need 0 and 1 in files, so we need to make a filter.'''\n",
    "            for item,label in zip(data,labels):\n",
    "                if label in [0,1]:\n",
    "                    all_data.append(item)\n",
    "                    all_labels.append(label)\n",
    "        self._data = np.vstack(all_data) #纵向组合成一个矩阵\n",
    "        self._data = self._data / 127.5 - 1   \n",
    "        self._labels = np.hstack(all_labels) #横向组合成一个矩阵\n",
    "        print(self._data.shape)\n",
    "        print(self._labels.shape)\n",
    "        self._num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "                \n",
    "    def _shuffle_data(self):\n",
    "        #[0,1,2,3,4,5] ->[3,4,2,5,1.0] 在训练集上做\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "        \n",
    "    def next_batch(self,batch_size):\n",
    "        \"\"\"return batch size examples as a batch\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more examples\")\n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator : end_indicator]\n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data , batch_labels\n",
    "\n",
    "    \n",
    "    '''至此处理完了cifar数据'''\n",
    "\n",
    "\n",
    "train_filemnames = [os.path.join(CIFAR_DIR,'data_batch_%d' % i) for i in range(1,6)]\n",
    "test_data_filenames = [os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "train_data = CifarData(train_filemnames,True)\n",
    "test_data = CifarData(test_data_filenames,False)\n",
    "\n",
    "batch_data, batch_labels = train_data.next_batch(10)\n",
    "print(batch_data)\n",
    "print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"在tensorflow里，要先把图搭建起来，再往里面填数据。\"\"\"\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32,[None,3072])  \n",
    "#这是一个占位符，也是一个变量，可以往里面塞数据。这是data的palceholder，None表示样本数是不确定的。\n",
    "\n",
    "y = tf.placeholder(tf.int64,[None])  \n",
    "#这是labels的placeholder，因为都是离散变量，所以用int64\n",
    "\n",
    "#（3072，1）\n",
    "w = tf.get_variable('w',[x.get_shape()[-1],1],\n",
    "                   initializer = tf.random_normal_initializer(0,1)) \n",
    "#w是要和x做内积的权重，w的大小就是x的大小中的3072，这是一个二分类器，所以输出为1，\n",
    "#还需要定义一个初始化的方法，这里使用比较通用的正态分布。均值是1，方差为1.\n",
    "\n",
    "\n",
    "#（1，）                                                                    \n",
    "b = tf.get_variable('b',[1],\n",
    "                   initializer =tf.constant_initializer(0.0) )  \n",
    "#b是偏置，大小为w的第二维度，也就是1，输出也是1，初始化方法使用常数初始化0.\n",
    "\n",
    "#[None,3072]*[3072,1] = [None,1]\n",
    "y_ = tf.matmul(x,w) + b # y_是x和w做内积加上偏置b后的结果。因为x和w在多样本下都是矩阵，所以用matmul矩阵乘法\n",
    "\n",
    "#[None,1]\n",
    "p_y_1 = tf.nn.sigmoid(y_)  #把y_转换成概率值，就是放到sigmoid函数中。这是y=1的概率\n",
    "\n",
    "'''为了把y和p_y_1和y做差别，所以要让他们的shape和数值单位一致，所以要把y进行reshape'''\n",
    "y_reshaped = tf.reshape(y,(-1,1))\n",
    "y_reshaped_float = tf.cast(y_reshaped,tf.float32)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y_reshaped_float - p_y_1))  #损失函数的表达式，差的平方和的均值。\n",
    "\n",
    "\n",
    "\"\"\"我们还对准确率感兴趣，所以现在来计算准确率\"\"\"\n",
    "#bool\n",
    "predict = p_y_1 > 0.5  #以这个概率大于百分之五十为正确，记为1.\n",
    "\n",
    "#[1,0,1,1,1,0,1]类似的\n",
    "correct_prediction = tf.equal(tf.cast(predict,tf.int64),y_reshaped) \n",
    "#这里指的是我们的模型预测出的值是否正确的布尔值转换成int后的值和y_reshaped是否相等的布尔值\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float64)) #准确率就是我们上面所说的这个值的平均值\n",
    "\n",
    "'''定义一个梯度下降的方法'''\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss) \n",
    "    #AdamOptimizer是梯度下降的一个变种，1e-3是一个初始化的generate，并且我们优化的是loss值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 500, loss:0.19298, acc:0.80000\n",
      "[Train] Step: 1000, loss:0.22279, acc:0.75000\n",
      "[Train] Step: 1500, loss:0.15425, acc:0.85000\n",
      "[Train] Step: 2000, loss:0.30007, acc:0.70000\n",
      "[Train] Step: 2500, loss:0.22056, acc:0.75000\n",
      "[Train] Step: 3000, loss:0.21085, acc:0.75000\n",
      "[Train] Step: 3500, loss:0.30349, acc:0.70000\n",
      "[Train] Step: 4000, loss:0.14557, acc:0.85000\n",
      "[Train] Step: 4500, loss:0.29734, acc:0.70000\n",
      "[Train] Step: 5000, loss:0.17265, acc:0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 5000, acc:0.80650\n",
      "[Train] Step: 5500, loss:0.04424, acc:0.95000\n",
      "[Train] Step: 6000, loss:0.06072, acc:0.95000\n",
      "[Train] Step: 6500, loss:0.25406, acc:0.70000\n",
      "[Train] Step: 7000, loss:0.19669, acc:0.80000\n",
      "[Train] Step: 7500, loss:0.09957, acc:0.90000\n",
      "[Train] Step: 8000, loss:0.05000, acc:0.95000\n",
      "[Train] Step: 8500, loss:0.19999, acc:0.80000\n",
      "[Train] Step: 9000, loss:0.12714, acc:0.85000\n",
      "[Train] Step: 9500, loss:0.15000, acc:0.85000\n",
      "[Train] Step: 10000, loss:0.15020, acc:0.85000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 10000, acc:0.81650\n",
      "[Train] Step: 10500, loss:0.08772, acc:0.90000\n",
      "[Train] Step: 11000, loss:0.21444, acc:0.75000\n",
      "[Train] Step: 11500, loss:0.25909, acc:0.75000\n",
      "[Train] Step: 12000, loss:0.24965, acc:0.75000\n",
      "[Train] Step: 12500, loss:0.15008, acc:0.85000\n",
      "[Train] Step: 13000, loss:0.14929, acc:0.85000\n",
      "[Train] Step: 13500, loss:0.05000, acc:0.95000\n",
      "[Train] Step: 14000, loss:0.19404, acc:0.80000\n",
      "[Train] Step: 14500, loss:0.05442, acc:0.95000\n",
      "[Train] Step: 15000, loss:0.29720, acc:0.70000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 15000, acc:0.82000\n",
      "[Train] Step: 15500, loss:0.14999, acc:0.85000\n",
      "[Train] Step: 16000, loss:0.30128, acc:0.70000\n",
      "[Train] Step: 16500, loss:0.19999, acc:0.80000\n",
      "[Train] Step: 17000, loss:0.16340, acc:0.80000\n",
      "[Train] Step: 17500, loss:0.05386, acc:0.95000\n",
      "[Train] Step: 18000, loss:0.10908, acc:0.90000\n",
      "[Train] Step: 18500, loss:0.00001, acc:1.00000\n",
      "[Train] Step: 19000, loss:0.15078, acc:0.85000\n",
      "[Train] Step: 19500, loss:0.19573, acc:0.80000\n",
      "[Train] Step: 20000, loss:0.09901, acc:0.90000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 20000, acc:0.81850\n",
      "[Train] Step: 20500, loss:0.23416, acc:0.75000\n",
      "[Train] Step: 21000, loss:0.15014, acc:0.85000\n",
      "[Train] Step: 21500, loss:0.10029, acc:0.90000\n",
      "[Train] Step: 22000, loss:0.16591, acc:0.80000\n",
      "[Train] Step: 22500, loss:0.00478, acc:1.00000\n",
      "[Train] Step: 23000, loss:0.10000, acc:0.90000\n",
      "[Train] Step: 23500, loss:0.15000, acc:0.85000\n",
      "[Train] Step: 24000, loss:0.05054, acc:0.95000\n",
      "[Train] Step: 24500, loss:0.30658, acc:0.70000\n",
      "[Train] Step: 25000, loss:0.15069, acc:0.85000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 25000, acc:0.82600\n",
      "[Train] Step: 25500, loss:0.25005, acc:0.75000\n",
      "[Train] Step: 26000, loss:0.20000, acc:0.80000\n",
      "[Train] Step: 26500, loss:0.15003, acc:0.85000\n",
      "[Train] Step: 27000, loss:0.19659, acc:0.80000\n",
      "[Train] Step: 27500, loss:0.10002, acc:0.90000\n",
      "[Train] Step: 28000, loss:0.12436, acc:0.85000\n",
      "[Train] Step: 28500, loss:0.15002, acc:0.85000\n",
      "[Train] Step: 29000, loss:0.13016, acc:0.85000\n",
      "[Train] Step: 29500, loss:0.20020, acc:0.80000\n",
      "[Train] Step: 30000, loss:0.18287, acc:0.80000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 30000, acc:0.82350\n",
      "[Train] Step: 30500, loss:0.15307, acc:0.85000\n",
      "[Train] Step: 31000, loss:0.10002, acc:0.90000\n",
      "[Train] Step: 31500, loss:0.05325, acc:0.95000\n",
      "[Train] Step: 32000, loss:0.20262, acc:0.80000\n",
      "[Train] Step: 32500, loss:0.10000, acc:0.90000\n",
      "[Train] Step: 33000, loss:0.10049, acc:0.90000\n",
      "[Train] Step: 33500, loss:0.15000, acc:0.85000\n",
      "[Train] Step: 34000, loss:0.20090, acc:0.80000\n",
      "[Train] Step: 34500, loss:0.09865, acc:0.90000\n",
      "[Train] Step: 35000, loss:0.05000, acc:0.95000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 35000, acc:0.82250\n",
      "[Train] Step: 35500, loss:0.30000, acc:0.70000\n",
      "[Train] Step: 36000, loss:0.15114, acc:0.85000\n",
      "[Train] Step: 36500, loss:0.15621, acc:0.85000\n",
      "[Train] Step: 37000, loss:0.15513, acc:0.85000\n",
      "[Train] Step: 37500, loss:0.10177, acc:0.90000\n",
      "[Train] Step: 38000, loss:0.15442, acc:0.85000\n",
      "[Train] Step: 38500, loss:0.05650, acc:0.95000\n",
      "[Train] Step: 39000, loss:0.20060, acc:0.80000\n",
      "[Train] Step: 39500, loss:0.13179, acc:0.85000\n",
      "[Train] Step: 40000, loss:0.05005, acc:0.95000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 40000, acc:0.82400\n",
      "[Train] Step: 40500, loss:0.05022, acc:0.95000\n",
      "[Train] Step: 41000, loss:0.10109, acc:0.90000\n",
      "[Train] Step: 41500, loss:0.39987, acc:0.60000\n",
      "[Train] Step: 42000, loss:0.20781, acc:0.80000\n",
      "[Train] Step: 42500, loss:0.00010, acc:1.00000\n",
      "[Train] Step: 43000, loss:0.20223, acc:0.80000\n",
      "[Train] Step: 43500, loss:0.10000, acc:0.90000\n",
      "[Train] Step: 44000, loss:0.05000, acc:0.95000\n",
      "[Train] Step: 44500, loss:0.15002, acc:0.85000\n",
      "[Train] Step: 45000, loss:0.39996, acc:0.60000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 45000, acc:0.82350\n",
      "[Train] Step: 45500, loss:0.34926, acc:0.65000\n",
      "[Train] Step: 46000, loss:0.10003, acc:0.90000\n",
      "[Train] Step: 46500, loss:0.15019, acc:0.85000\n",
      "[Train] Step: 47000, loss:0.00000, acc:1.00000\n",
      "[Train] Step: 47500, loss:0.10223, acc:0.90000\n",
      "[Train] Step: 48000, loss:0.09960, acc:0.90000\n",
      "[Train] Step: 48500, loss:0.24572, acc:0.75000\n",
      "[Train] Step: 49000, loss:0.14989, acc:0.85000\n",
      "[Train] Step: 49500, loss:0.12053, acc:0.85000\n",
      "[Train] Step: 50000, loss:0.25017, acc:0.75000\n",
      "(2000, 3072)\n",
      "(2000,)\n",
      "[Test] Steps: 50000, acc:0.82350\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() \n",
    "# 构建一个初始化函数，用于初始化数据\n",
    "batch_size = 20\n",
    "train_steps = 50000\n",
    "test_steps = 100\n",
    "'''构建一个session，用于执行计算图'''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(train_steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "        loss_val, acc_val, _ = sess.run(\n",
    "            [loss, accuracy, train_op],\n",
    "            feed_dict={\n",
    "                x: batch_data,\n",
    "                y: batch_labels})\n",
    "        if (i+1) % 500 == 0:\n",
    "            print('[Train] Step: %d, loss:%4.5f, acc:%4.5f'\\\n",
    "                 %(i+1,loss_val,acc_val))\n",
    "        if (i+1) % 5000 == 0:\n",
    "            test_data = CifarData(test_data_filenames, False)\n",
    "            all_test_acc_val =[]\n",
    "            for j in range(test_steps):\n",
    "                test_batch_data, test_batch_labels = test_data.next_batch(batch_size)\n",
    "                test_acc_val = sess.run([accuracy],feed_dict = {x:test_batch_data,y:test_batch_labels})\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc = np.mean(all_test_acc_val)\n",
    "            print('[Test] Steps: %d, acc:%4.5f' %(i+1, test_acc))\n",
    "#通过调用session的run函数执行计算图。可以计算loss, accuracy, train_op。加了train_op就是在训练，没加就只是在测试。\n",
    "#feed_dict是我们输入的数据，x是cifar10的图片数据，y是label数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
