{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_batch_1', 'readme.html', 'batches.meta', 'data_batch_2', 'data_batch_5', 'test_batch', 'data_batch_4', 'data_batch_3']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "CIFAR_DIR = \"./cifar-10-batches-py\"\n",
    "print(os.listdir(CIFAR_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n",
      "(50000,)\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[[-0.34901961 -0.54509804 -0.39607843 ... -0.00392157 -0.06666667\n",
      "  -0.22352941]\n",
      " [-0.19215686 -0.21568627 -0.27843137 ...  0.4745098   0.49019608\n",
      "   0.49803922]\n",
      " [ 0.45098039  0.46666667  0.56078431 ... -0.40392157 -0.41960784\n",
      "  -0.39607843]\n",
      " ...\n",
      " [ 0.52156863  0.51372549  0.45882353 ... -0.15294118 -0.18431373\n",
      "  -0.2       ]\n",
      " [-0.7254902  -0.7254902  -0.70980392 ... -0.56862745 -0.57647059\n",
      "  -0.57647059]\n",
      " [-0.79607843 -0.56862745 -0.17647059 ... -0.22352941 -0.16862745\n",
      "  -0.14509804]]\n",
      "[4 6 6 0 1 4 3 5 5 4]\n"
     ]
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"load data from data file.\"\"\"\n",
    "    with open(filename,'rb') as f:\n",
    "        data = pickle.load(f,encoding = 'latin')\n",
    "        return data['data'], data['labels']\n",
    "    \n",
    "class CifarData:\n",
    "    \"\"\"定义一个类处理cifar的数据\"\"\"\n",
    "    def __init__(self, filenames, need_shuffle): #对于训练集来说需要shuffle，打乱数据以增加训练成效，对于测试集来说就不需要。\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        for filename in filenames:\n",
    "            data,labels = load_data(filename) \n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)\n",
    "        self._data = np.vstack(all_data) #纵向组合成一个矩阵\n",
    "        self._data = self._data / 127.5 - 1   \n",
    "        self._labels = np.hstack(all_labels) #横向组合成一个矩阵\n",
    "        print(self._data.shape)\n",
    "        print(self._labels.shape)\n",
    "        self._num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "                \n",
    "    def _shuffle_data(self):\n",
    "        #[0,1,2,3,4,5] ->[3,4,2,5,1.0] 在训练集上做\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "        \n",
    "    def next_batch(self,batch_size):\n",
    "        \"\"\"return batch size examples as a batch\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more examples\")\n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator : end_indicator]\n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data , batch_labels\n",
    "\n",
    "    \n",
    "    '''至此处理完了cifar数据'''\n",
    "\n",
    "\n",
    "train_filemnames = [os.path.join(CIFAR_DIR,'data_batch_%d' % i) for i in range(1,6)]\n",
    "test_data_filenames = [os.path.join(CIFAR_DIR, 'test_batch')]\n",
    "\n",
    "train_data = CifarData(train_filemnames,True)\n",
    "test_data = CifarData(test_data_filenames,False)\n",
    "\n",
    "batch_data, batch_labels = train_data.next_batch(10)\n",
    "print(batch_data)\n",
    "print(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"在tensorflow里，要先把图搭建起来，再往里面填数据。\"\"\"\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32,[None,3072])  \n",
    "#这是一个占位符，也是一个变量，可以往里面塞数据。这是data的palceholder，None表示样本数是不确定的。\n",
    "\n",
    "y = tf.placeholder(tf.int64,[None])  \n",
    "#这是labels的placeholder，因为都是离散变量，所以用int64\n",
    "\n",
    "#（3072，10）\n",
    "w = tf.get_variable('w',[x.get_shape()[-1],10],\n",
    "                   initializer = tf.random_normal_initializer(0,1)) \n",
    "#w是要和x做内积的权重，w的大小就是x的大小中的3072，这是一个多分类器，所以输出为10，\n",
    "#还需要定义一个初始化的方法，这里使用比较通用的正态分布。均值是1，方差为1.\n",
    "\n",
    "\n",
    "#（10，）                                                                    \n",
    "b = tf.get_variable('b',[10],\n",
    "                   initializer =tf.constant_initializer(0.0) )  \n",
    "#b是偏置，大小为w的第二维度，也就是1，输出也是1，初始化方法使用常数初始化0.\n",
    "\n",
    "#[None,3072]*[3072,10] = [None,10]\n",
    "y_ = tf.matmul(x,w) + b \n",
    "# y_是x和w做内积加上偏置b后的结果。因为x和w在多样本下都是矩阵，所以用matmul矩阵乘法\n",
    "'''\n",
    "#mean square loss\n",
    "p_y = tf.nn.softmax(y_)\n",
    "# course: 1 + e^x\n",
    "# api: e^x / sum(e^x)\n",
    "#[[0.01,0.03,0.4,...,0.2],[...]] 这是labels的一个概率分布，为了与之对应要把y也变成一个概率分布，用onehot算法。\n",
    "y_one_hot = tf.one_hot(y,10,dtype = tf.float32)\n",
    "# 5-->[0,0,0,0,1,0,0,0,0,0]\n",
    "loss = tf.reduce_mean(tf.square(y_one_hot - p_y))\n",
    "#同样使用平方差公式计算损失值。'''\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels = y, logits = y_)\n",
    "#y_ ->softmax\n",
    "#y ->one_hot\n",
    "#loss = ylogy_\n",
    "\n",
    "\n",
    "\"\"\"我们还对准确率感兴趣，所以现在来计算准确率\"\"\"\n",
    "#indice\n",
    "predict = tf.argmax(y_,1)   #取每一行中最大的数\n",
    "\n",
    "#[1,0,1,1,1,0,1]类似的\n",
    "correct_prediction = tf.equal(predict,y) \n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float64)) #准确率就是我们上面所说的这个值的平均值\n",
    "\n",
    "'''定义一个梯度下降的方法'''\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss) \n",
    "    #AdamOptimizer是梯度下降的一个变种，1e-3是一个初始化的generate，并且我们优化的是loss值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 500, loss:0.15025, acc:0.15000\n",
      "[Train] Step: 1000, loss:0.10670, acc:0.45000\n",
      "[Train] Step: 1500, loss:0.14846, acc:0.25000\n",
      "[Train] Step: 2000, loss:0.12170, acc:0.35000\n",
      "[Train] Step: 2500, loss:0.13631, acc:0.30000\n",
      "[Train] Step: 3000, loss:0.14669, acc:0.25000\n",
      "[Train] Step: 3500, loss:0.11757, acc:0.40000\n",
      "[Train] Step: 4000, loss:0.12123, acc:0.40000\n",
      "[Train] Step: 4500, loss:0.11981, acc:0.40000\n",
      "[Train] Step: 5000, loss:0.13907, acc:0.30000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 5000, acc:0.31400\n",
      "[Train] Step: 5500, loss:0.12002, acc:0.40000\n",
      "[Train] Step: 6000, loss:0.15982, acc:0.20000\n",
      "[Train] Step: 6500, loss:0.10134, acc:0.45000\n",
      "[Train] Step: 7000, loss:0.16628, acc:0.15000\n",
      "[Train] Step: 7500, loss:0.13845, acc:0.30000\n",
      "[Train] Step: 8000, loss:0.12423, acc:0.30000\n",
      "[Train] Step: 8500, loss:0.14269, acc:0.25000\n",
      "[Train] Step: 9000, loss:0.14246, acc:0.25000\n",
      "[Train] Step: 9500, loss:0.09004, acc:0.55000\n",
      "[Train] Step: 10000, loss:0.09978, acc:0.50000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 10000, acc:0.35500\n",
      "[Train] Step: 10500, loss:0.13508, acc:0.30000\n",
      "[Train] Step: 11000, loss:0.13943, acc:0.30000\n",
      "[Train] Step: 11500, loss:0.10724, acc:0.40000\n",
      "[Train] Step: 12000, loss:0.12762, acc:0.35000\n",
      "[Train] Step: 12500, loss:0.12582, acc:0.35000\n",
      "[Train] Step: 13000, loss:0.10810, acc:0.45000\n",
      "[Train] Step: 13500, loss:0.12831, acc:0.35000\n",
      "[Train] Step: 14000, loss:0.13748, acc:0.30000\n",
      "[Train] Step: 14500, loss:0.10690, acc:0.45000\n",
      "[Train] Step: 15000, loss:0.09536, acc:0.50000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 15000, acc:0.35850\n",
      "[Train] Step: 15500, loss:0.13686, acc:0.30000\n",
      "[Train] Step: 16000, loss:0.12304, acc:0.35000\n",
      "[Train] Step: 16500, loss:0.12899, acc:0.35000\n",
      "[Train] Step: 17000, loss:0.10443, acc:0.45000\n",
      "[Train] Step: 17500, loss:0.07834, acc:0.60000\n",
      "[Train] Step: 18000, loss:0.14845, acc:0.25000\n",
      "[Train] Step: 18500, loss:0.13658, acc:0.30000\n",
      "[Train] Step: 19000, loss:0.13792, acc:0.30000\n",
      "[Train] Step: 19500, loss:0.13737, acc:0.30000\n",
      "[Train] Step: 20000, loss:0.09577, acc:0.50000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 20000, acc:0.36800\n",
      "[Train] Step: 20500, loss:0.12302, acc:0.35000\n",
      "[Train] Step: 21000, loss:0.11092, acc:0.45000\n",
      "[Train] Step: 21500, loss:0.12970, acc:0.35000\n",
      "[Train] Step: 22000, loss:0.12525, acc:0.35000\n",
      "[Train] Step: 22500, loss:0.14903, acc:0.25000\n",
      "[Train] Step: 23000, loss:0.15722, acc:0.20000\n",
      "[Train] Step: 23500, loss:0.13394, acc:0.30000\n",
      "[Train] Step: 24000, loss:0.11751, acc:0.40000\n",
      "[Train] Step: 24500, loss:0.12658, acc:0.35000\n",
      "[Train] Step: 25000, loss:0.13549, acc:0.30000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 25000, acc:0.37250\n",
      "[Train] Step: 25500, loss:0.12333, acc:0.35000\n",
      "[Train] Step: 26000, loss:0.12481, acc:0.35000\n",
      "[Train] Step: 26500, loss:0.12746, acc:0.35000\n",
      "[Train] Step: 27000, loss:0.11770, acc:0.40000\n",
      "[Train] Step: 27500, loss:0.12180, acc:0.35000\n",
      "[Train] Step: 28000, loss:0.12687, acc:0.35000\n",
      "[Train] Step: 28500, loss:0.11757, acc:0.40000\n",
      "[Train] Step: 29000, loss:0.09951, acc:0.50000\n",
      "[Train] Step: 29500, loss:0.11337, acc:0.40000\n",
      "[Train] Step: 30000, loss:0.11737, acc:0.40000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 30000, acc:0.36950\n",
      "[Train] Step: 30500, loss:0.11817, acc:0.40000\n",
      "[Train] Step: 31000, loss:0.15416, acc:0.20000\n",
      "[Train] Step: 31500, loss:0.13925, acc:0.30000\n",
      "[Train] Step: 32000, loss:0.14923, acc:0.25000\n",
      "[Train] Step: 32500, loss:0.11443, acc:0.40000\n",
      "[Train] Step: 33000, loss:0.09762, acc:0.50000\n",
      "[Train] Step: 33500, loss:0.15732, acc:0.20000\n",
      "[Train] Step: 34000, loss:0.08434, acc:0.60000\n",
      "[Train] Step: 34500, loss:0.10113, acc:0.45000\n",
      "[Train] Step: 35000, loss:0.13968, acc:0.30000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 35000, acc:0.38000\n",
      "[Train] Step: 35500, loss:0.12629, acc:0.35000\n",
      "[Train] Step: 36000, loss:0.12793, acc:0.35000\n",
      "[Train] Step: 36500, loss:0.14760, acc:0.25000\n",
      "[Train] Step: 37000, loss:0.09751, acc:0.50000\n",
      "[Train] Step: 37500, loss:0.08166, acc:0.55000\n",
      "[Train] Step: 38000, loss:0.11895, acc:0.40000\n",
      "[Train] Step: 38500, loss:0.07967, acc:0.60000\n",
      "[Train] Step: 39000, loss:0.12069, acc:0.35000\n",
      "[Train] Step: 39500, loss:0.11692, acc:0.40000\n",
      "[Train] Step: 40000, loss:0.12967, acc:0.35000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 40000, acc:0.38100\n",
      "[Train] Step: 40500, loss:0.10622, acc:0.45000\n",
      "[Train] Step: 41000, loss:0.14568, acc:0.25000\n",
      "[Train] Step: 41500, loss:0.11746, acc:0.40000\n",
      "[Train] Step: 42000, loss:0.11896, acc:0.40000\n",
      "[Train] Step: 42500, loss:0.11569, acc:0.40000\n",
      "[Train] Step: 43000, loss:0.15770, acc:0.20000\n",
      "[Train] Step: 43500, loss:0.09638, acc:0.50000\n",
      "[Train] Step: 44000, loss:0.12881, acc:0.35000\n",
      "[Train] Step: 44500, loss:0.10093, acc:0.50000\n",
      "[Train] Step: 45000, loss:0.10743, acc:0.45000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 45000, acc:0.38250\n",
      "[Train] Step: 45500, loss:0.10526, acc:0.45000\n",
      "[Train] Step: 46000, loss:0.11980, acc:0.40000\n",
      "[Train] Step: 46500, loss:0.13307, acc:0.30000\n",
      "[Train] Step: 47000, loss:0.09420, acc:0.50000\n",
      "[Train] Step: 47500, loss:0.13307, acc:0.30000\n",
      "[Train] Step: 48000, loss:0.11296, acc:0.40000\n",
      "[Train] Step: 48500, loss:0.08000, acc:0.60000\n",
      "[Train] Step: 49000, loss:0.11579, acc:0.40000\n",
      "[Train] Step: 49500, loss:0.14695, acc:0.25000\n",
      "[Train] Step: 50000, loss:0.08871, acc:0.55000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 50000, acc:0.38500\n",
      "[Train] Step: 50500, loss:0.11390, acc:0.40000\n",
      "[Train] Step: 51000, loss:0.10749, acc:0.45000\n",
      "[Train] Step: 51500, loss:0.14240, acc:0.25000\n",
      "[Train] Step: 52000, loss:0.10181, acc:0.45000\n",
      "[Train] Step: 52500, loss:0.12708, acc:0.35000\n",
      "[Train] Step: 53000, loss:0.07100, acc:0.60000\n",
      "[Train] Step: 53500, loss:0.11232, acc:0.45000\n",
      "[Train] Step: 54000, loss:0.12401, acc:0.35000\n",
      "[Train] Step: 54500, loss:0.12783, acc:0.35000\n",
      "[Train] Step: 55000, loss:0.11780, acc:0.40000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 55000, acc:0.38850\n",
      "[Train] Step: 55500, loss:0.11947, acc:0.40000\n",
      "[Train] Step: 56000, loss:0.09028, acc:0.55000\n",
      "[Train] Step: 56500, loss:0.15995, acc:0.20000\n",
      "[Train] Step: 57000, loss:0.13010, acc:0.30000\n",
      "[Train] Step: 57500, loss:0.12727, acc:0.35000\n",
      "[Train] Step: 58000, loss:0.11121, acc:0.45000\n",
      "[Train] Step: 58500, loss:0.10534, acc:0.45000\n",
      "[Train] Step: 59000, loss:0.07747, acc:0.60000\n",
      "[Train] Step: 59500, loss:0.11613, acc:0.35000\n",
      "[Train] Step: 60000, loss:0.07655, acc:0.60000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 60000, acc:0.38450\n",
      "[Train] Step: 60500, loss:0.07982, acc:0.60000\n",
      "[Train] Step: 61000, loss:0.07446, acc:0.60000\n",
      "[Train] Step: 61500, loss:0.10731, acc:0.45000\n",
      "[Train] Step: 62000, loss:0.08792, acc:0.55000\n",
      "[Train] Step: 62500, loss:0.11396, acc:0.40000\n",
      "[Train] Step: 63000, loss:0.13634, acc:0.30000\n",
      "[Train] Step: 63500, loss:0.07649, acc:0.60000\n",
      "[Train] Step: 64000, loss:0.12600, acc:0.35000\n",
      "[Train] Step: 64500, loss:0.15093, acc:0.20000\n",
      "[Train] Step: 65000, loss:0.11117, acc:0.40000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 65000, acc:0.38650\n",
      "[Train] Step: 65500, loss:0.09954, acc:0.50000\n",
      "[Train] Step: 66000, loss:0.11894, acc:0.40000\n",
      "[Train] Step: 66500, loss:0.06681, acc:0.65000\n",
      "[Train] Step: 67000, loss:0.12981, acc:0.35000\n",
      "[Train] Step: 67500, loss:0.11739, acc:0.40000\n",
      "[Train] Step: 68000, loss:0.12801, acc:0.35000\n",
      "[Train] Step: 68500, loss:0.08995, acc:0.55000\n",
      "[Train] Step: 69000, loss:0.07002, acc:0.65000\n",
      "[Train] Step: 69500, loss:0.12581, acc:0.35000\n",
      "[Train] Step: 70000, loss:0.15759, acc:0.20000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 70000, acc:0.39500\n",
      "[Train] Step: 70500, loss:0.12618, acc:0.35000\n",
      "[Train] Step: 71000, loss:0.11960, acc:0.40000\n",
      "[Train] Step: 71500, loss:0.10781, acc:0.45000\n",
      "[Train] Step: 72000, loss:0.09587, acc:0.50000\n",
      "[Train] Step: 72500, loss:0.10574, acc:0.45000\n",
      "[Train] Step: 73000, loss:0.09200, acc:0.55000\n",
      "[Train] Step: 73500, loss:0.12981, acc:0.35000\n",
      "[Train] Step: 74000, loss:0.13707, acc:0.30000\n",
      "[Train] Step: 74500, loss:0.11740, acc:0.40000\n",
      "[Train] Step: 75000, loss:0.13743, acc:0.30000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 75000, acc:0.38400\n",
      "[Train] Step: 75500, loss:0.07717, acc:0.60000\n",
      "[Train] Step: 76000, loss:0.11736, acc:0.40000\n",
      "[Train] Step: 76500, loss:0.07917, acc:0.60000\n",
      "[Train] Step: 77000, loss:0.08691, acc:0.55000\n",
      "[Train] Step: 77500, loss:0.12090, acc:0.30000\n",
      "[Train] Step: 78000, loss:0.11727, acc:0.40000\n",
      "[Train] Step: 78500, loss:0.11730, acc:0.40000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Step: 79000, loss:0.08384, acc:0.55000\n",
      "[Train] Step: 79500, loss:0.08981, acc:0.55000\n",
      "[Train] Step: 80000, loss:0.08925, acc:0.55000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 80000, acc:0.38300\n",
      "[Train] Step: 80500, loss:0.13578, acc:0.30000\n",
      "[Train] Step: 81000, loss:0.10975, acc:0.45000\n",
      "[Train] Step: 81500, loss:0.09052, acc:0.55000\n",
      "[Train] Step: 82000, loss:0.13648, acc:0.30000\n",
      "[Train] Step: 82500, loss:0.16815, acc:0.15000\n",
      "[Train] Step: 83000, loss:0.10848, acc:0.45000\n",
      "[Train] Step: 83500, loss:0.11955, acc:0.40000\n",
      "[Train] Step: 84000, loss:0.09706, acc:0.50000\n",
      "[Train] Step: 84500, loss:0.11832, acc:0.40000\n",
      "[Train] Step: 85000, loss:0.10403, acc:0.45000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 85000, acc:0.39950\n",
      "[Train] Step: 85500, loss:0.06000, acc:0.70000\n",
      "[Train] Step: 86000, loss:0.09760, acc:0.50000\n",
      "[Train] Step: 86500, loss:0.14718, acc:0.25000\n",
      "[Train] Step: 87000, loss:0.11916, acc:0.40000\n",
      "[Train] Step: 87500, loss:0.08228, acc:0.60000\n",
      "[Train] Step: 88000, loss:0.07831, acc:0.60000\n",
      "[Train] Step: 88500, loss:0.10923, acc:0.45000\n",
      "[Train] Step: 89000, loss:0.11665, acc:0.40000\n",
      "[Train] Step: 89500, loss:0.08367, acc:0.55000\n",
      "[Train] Step: 90000, loss:0.06151, acc:0.70000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 90000, acc:0.39150\n",
      "[Train] Step: 90500, loss:0.10914, acc:0.45000\n",
      "[Train] Step: 91000, loss:0.12562, acc:0.35000\n",
      "[Train] Step: 91500, loss:0.10655, acc:0.45000\n",
      "[Train] Step: 92000, loss:0.07998, acc:0.60000\n",
      "[Train] Step: 92500, loss:0.07774, acc:0.60000\n",
      "[Train] Step: 93000, loss:0.09931, acc:0.50000\n",
      "[Train] Step: 93500, loss:0.09810, acc:0.50000\n",
      "[Train] Step: 94000, loss:0.07158, acc:0.55000\n",
      "[Train] Step: 94500, loss:0.13931, acc:0.30000\n",
      "[Train] Step: 95000, loss:0.11971, acc:0.40000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 95000, acc:0.38600\n",
      "[Train] Step: 95500, loss:0.09542, acc:0.50000\n",
      "[Train] Step: 96000, loss:0.10166, acc:0.50000\n",
      "[Train] Step: 96500, loss:0.12628, acc:0.35000\n",
      "[Train] Step: 97000, loss:0.09652, acc:0.50000\n",
      "[Train] Step: 97500, loss:0.12817, acc:0.35000\n",
      "[Train] Step: 98000, loss:0.12694, acc:0.35000\n",
      "[Train] Step: 98500, loss:0.10655, acc:0.40000\n",
      "[Train] Step: 99000, loss:0.16023, acc:0.15000\n",
      "[Train] Step: 99500, loss:0.12788, acc:0.35000\n",
      "[Train] Step: 100000, loss:0.12718, acc:0.35000\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "[Test] Steps: 100000, acc:0.37900\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() \n",
    "# 构建一个初始化函数，用于初始化数据\n",
    "batch_size = 20\n",
    "train_steps = 100000\n",
    "test_steps = 100\n",
    "'''构建一个session，用于执行计算图'''\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(train_steps):\n",
    "        batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "        loss_val, acc_val, _ = sess.run(\n",
    "            [loss, accuracy, train_op],\n",
    "            feed_dict={\n",
    "                x: batch_data,\n",
    "                y: batch_labels})\n",
    "        if (i+1) % 500 == 0:\n",
    "            print('[Train] Step: %d, loss:%4.5f, acc:%4.5f'\\\n",
    "                 %(i+1,loss_val,acc_val))\n",
    "        if (i+1) % 5000 == 0:\n",
    "            test_data = CifarData(test_data_filenames, False)\n",
    "            all_test_acc_val =[]\n",
    "            for j in range(test_steps):\n",
    "                test_batch_data, test_batch_labels = test_data.next_batch(batch_size)\n",
    "                test_acc_val = sess.run([accuracy],feed_dict = {x:test_batch_data,y:test_batch_labels})\n",
    "                all_test_acc_val.append(test_acc_val)\n",
    "            test_acc = np.mean(all_test_acc_val)\n",
    "            print('[Test] Steps: %d, acc:%4.5f' %(i+1, test_acc))\n",
    "#通过调用session的run函数执行计算图。可以计算loss, accuracy, train_op。加了train_op就是在训练，没加就只是在测试。\n",
    "#feed_dict是我们输入的数据，x是cifar10的图片数据，y是label数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
